\documentclass{beamer}
\usetheme{Boadilla}
\usecolortheme{whale}

\usepackage{tikz}
\tikzset{
  font={\fontsize{8pt}{10}\selectfont}}
\usetikzlibrary{calc}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{matrix}

\definecolor{clstackoverflow}{HTML}{FE7A15}  
\definecolor{clgithub}{HTML}{4078C0} 

\graphicspath{ {../img/} }

\title{Convolutional neural networks
for classification of transmission
electron microscopy imagery}

\subtitle{}

\author{Sergii Gryshkevych}
\institute{
Uppsala University \\
Supervisor: Max Pihlstr{\"o}m \\
Reviewer: Ida-Maria Sintorn \\
Examinator: Justin Pearson \\
Opponent: Christopher Lagerhult
}
\date{December 14, 2016}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction}


\begin{frame}
\frametitle{Introduction}
This MSc project is done in cooperation with Vironova AB.
\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{vironova_logo.jpg} 
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Objective}
\begin{itemize}
\item<1-> Discuss the suitability of applying CNN method for classification of electron microscopy images
\item<2-> Benchmark CNN models against SVM classifier for the selected problems 
\item<3-> Discuss Deep Learning software:
\begin{itemize}
\item OS availability
\item Licenses
\item Performance
\item Community support
\end{itemize} 
\end{itemize}
\end{frame}


%
%	Problem description
%

\begin{frame}
\frametitle{Problem description: Lamellarity}

\begin{columns}
\column{0.35\textwidth}
Determine structure of a liposome according to the number of lamellae.
\vskip 0.2in
There are 14169 EM images and three classes:
\begin{itemize}
\item Unilamellar \\ 12368, 87.29\%
\item Multilamellar \\ 1717, 12.12\%
\item Uncertain \\ 84, 0.5\%
\end{itemize}
\column{0.65\textwidth}
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain2} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni3} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi3} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain3} \\
Unilamellar & Multilamellar & Uncertain \\[6pt]
\end{tabular}
\end{figure}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Problem description: Encapsulation}

\begin{columns}
\column{0.35\textwidth}
Determine presence of a liposomal encapsulation.
\vskip 0.2in
There are $24918$ EM images and three classes:
\begin{itemize}
\item Full \\ 24255, 97.34\%
\item Empty \\ 161, 2.01\%
\item Uncertain \\ 502, 0.65\%
\end{itemize}
\column{0.65\textwidth}
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	\includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/full} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/empty} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/uncertain} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/full2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/empty2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/uncertain2} \\
Full & Empty & Uncertain \\[6pt]
\end{tabular}
\end{figure}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{The data set of image features}

%\begin{block}{Image Feature}
%It is a piece of information which is relevant for solving the computational task %related to a certain application. Features may be specific structures in the image such %as points, edges or objects. Features may also be the result of a general neighborhood %operation or feature detection applied to the image.
%\end{block}
\begin{itemize}
\item Maximum width
\item Diameter
\item Length
\item Histogram
\item Image Moments 
\item Radial Density Profile
\item Edge Density Profile
\item Signal to noise
\end{itemize}

\begin{block}{}
SVM operates on feature representation of the image.
\end{block}
\end{frame}

%
%	Convolutional neural networks
%

\begin{frame}
\frametitle{Convolutional neural networks (CNN)}
\begin{block}{Convolutional neural network (CNN)}
It is a special kind of neural network for processing data that has a known, grid-like topology. CNN are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.
\end{block}

Why CNN?
\begin{itemize}
\item Scalability due to the following assumptions:
\begin{itemize}
\item Local connectivity
\item Parameter sharing
\end{itemize}
\item CNN operate on raw pixel data, i.e. minimum preprocessing
\item CNN learn image features themselves, i.e. do not need expert knowledge for selecting feature
\item Documented success
\end{itemize}
\end{frame}

%
%	The Class Imbalance Problem
%

\begin{frame}
\frametitle{The Class Imbalance Problem}

Lamellarity and Encapsulation data sets are imbalanced and it is a problem!
\vskip 0.2in
How to mitigate the class imbalance problem? I tried:
\begin{itemize}
\item Oversampling
\item Undersampling
\item SMOTE (Synthetic Minority Oversampling Technique)
\item Artificial data
\item Higher penalties for misclassification of minority classes
\end{itemize}


\end{frame}

%
%	Regularization
%

\begin{frame}
\frametitle{Regularization}

\begin{definition}
\textit{Regularization} is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error
\end{definition}

\begin{itemize}
\item Weight decay
\item Noise injection: label smoothing
\item Dropout
\item Early stopping
\item Data augmentation
\end{itemize}

\end{frame}

%
%	Data augmentation
%

%\section{Data augmentation}
%\begin{frame}
%\frametitle{Data augmentation techniques}
%
%Data augmentation introduces additional diversity in the training set by applying different transformations to the existing examples.
%\vskip 0.2in
%\begin{itemize}
%\item Rotation in the range $[-180, 180]$ degrees with spline interpolation
%\item Shear transformation in the range $[0, 0.2]$
%\item Vertical shift in the range $[-10, 10]$ percent of total height
%\item Horizontal shift in the range $[-10, 10]$ percent of total width
%\item Zoom in the range $[0.8, 1.0]$ which means zoom by a maximum $20\%$
%\item Horizontal flip
%\item Vertical flip
%\end{itemize}
%
%\end{frame}

\begin{frame}
\frametitle{Data augmentation example}
\begin{figure}
\centering
\begin{tabular}{ccccc}
\includegraphics[scale=0.5]{augmented/541253.jpg} & \includegraphics[scale=0.5]{augmented/_0_645.jpeg} & \includegraphics[scale=0.5]{augmented/_0_1385.jpeg} & \includegraphics[scale=0.5]{augmented/_0_1749.jpeg} & \includegraphics[scale=0.5]{augmented/_0_2343.jpeg} \\

\includegraphics[scale=0.5]{augmented/_0_4050.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3413.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3414.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3687.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3916.jpeg} \\
	
\includegraphics[scale=0.5]{augmented/_0_4165.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5014.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5165.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5567.jpeg} & \includegraphics[scale=0.5]{augmented/_0_6089.jpeg} \\

\includegraphics[scale=0.5]{augmented/_0_7140.jpeg} & \includegraphics[scale=0.5]{augmented/_0_7746.jpeg} & \includegraphics[scale=0.5]{augmented/_0_8553.jpeg} & \includegraphics[scale=0.5]{augmented/_0_8763.jpeg} & \includegraphics[scale=0.5]{augmented/_0_9361.jpeg} 
	
\end{tabular}
\end{figure}
\end{frame}

%
%	Input images modes
%

\begin{frame}
\frametitle{Input images: surrounding and masking}
Each image contains a liposome object and its surrounding which goes 50 pixels in each direction. Corresponding particle masks are also available. 
\vskip 0.2in
Three choices:
\begin{itemize}
\item Images with surrounding
\item Cropped images
\item \alert<2>{Cropped and masked} 
\end{itemize}
\end{frame}

%
%	Performance measures
%

\begin{frame}
\frametitle{Performance measures}

\begin{block}{}
Accuracy is not enough when the data set is imbalanced.
\end{block}

\begin{block}{Confusion matrix}
\begin{table}
\begin{tabular}{|c|c|c|}
\hline 
 & Predicted Positive & Predicted Negative \\ 
\hline 
Actual Positive & True Positive & False Negative \\ 
\hline 
Actual Negative & False Positive & True Negative \\ 
\hline 
\end{tabular} 
\end{table}
\end{block}

\begin{itemize}
\item True Positive Rate: $TPR = \frac{TP}{TP + FN}$ (sensitivity or recall)
\item True Negative Rate: $TNR = \frac{TN}{TN + FP}$ (specificity)
\item Positive Predicted Value: $PPV = \frac{TP}{TP + FP}$ (precision)
\item Negative Predicted Value: $NPV = \frac{TN}{TN + FN}$
\item $F_1$ score : harmonic mean of TPR and PPV
\end{itemize}
\end{frame}

%
%	LipNet networks
%

\begin{frame}
\frametitle{Network architectures}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{lipnet_architecture.png} 
\end{figure}

\end{frame}

\section{Results}
\begin{frame}
\frametitle{Which LipNet model is the best?}
Five LipNet models are evaluated by recording their 5-fold cross validated $F_1$ scores. 
\vskip 0.2in
\begin{itemize}
\item<2-> The Lamellarity Problem: \textbf{LipNet-4} is the best
\item<3-> The Encapsulation Problem: There is no clear leader. LipNet-6 performed worst. 
\end{itemize}
\end{frame}

%
%	CNN vs SVM
%

\section{Benchmarking}

\begin{frame}
\frametitle{CNN vs SVM: Lamellarity}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{cnn_vs_svm_lamellarity.png} 
\end{figure}

\begin{itemize}
\item CNN is slightly better than SVM.
\item More false negative \textit{unilamellar} by SVM than CNN.
\item More false positive \textit{multilamellar} by SVM than by CNN.
\item SVM is better in predicting \textit{uncertain}.
\item Many false positive predictions of \textit{uncertain}, mainly \textit{unilamellar} is confused with \textit{uncertain}.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{CNN vs SVM: Encapsulation}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{cnn_vs_svm_encapsulation.png} 
\end{figure}

\begin{itemize}
\item Almost the same performance.
\item Some \textit{full} are falsely classified as \textit{empty} and \textit{uncertain}
\begin{itemize}
\item Low NPV for \textit{full}
\item Poor precision (PPV) for \textit{empty} and \textit{uncertain}
\end{itemize}
\item PPV for \textit{full} and NPV for \textit{empty} and \textit{uncertain} are almost 1, so hardly any false positive of \textit{full}.
\end{itemize}

\end{frame}

%
%	Deep learning software
%

\begin{frame}
\frametitle{Deep learning software}

\begin{block}{OS and API}
The path of least resistance:
\begin{itemize}
\item Linux
\item Python
\end{itemize}
\end{block}

\begin{block}{Licensing}<2->
Nearly all libraries are distributed according to some of OSI-approved licenses like Apache, BSD, MIT, etc.
\end{block}

\begin{block}{GPU}<3->
All libraries benefit from GPUs. At the moment there is no study that shows superiority of any library in terms of performance.
\end{block}

\end{frame}

%
%	Popularity of DL software
%

\begin{frame}
\frametitle{Popularity of deep learning software as of October 2016}
\begin{figure}[H]
\centering

\begin{tikzpicture}
\begin{axis}[
	xbar,
	y axis line style = { opacity = 0 },
	axis x line = none,
	tickwidth = 0pt,
	enlarge y limits = 0.2,
	enlarge x limits = 0.02,
	bar width = 0.2cm,
	symbolic y coords = {Caffe, CNTK, Keras, Lasagne, MXNET, NOLEARN, TensorFlow, Theano, Torch},
	ytick = data,
	nodes near coords,
	height = 9cm,
	legend style={at={(1.1, 0.5)},anchor=north east}
    ]

% stackoverflow
\addplot[color=clstackoverflow, fill=clstackoverflow] coordinates{(1161,Caffe) (15,CNTK) (571,Keras) (168,Lasagne) (29,MXNET) (45,NOLEARN) (4000,TensorFlow) (1555,Theano) (533,Torch)};

% github
\addplot[color=clgithub, fill=clgithub] coordinates{(1605,Caffe) (746,CNTK) (667,Keras) (208,Lasagne) (615,MXNET) (54,NOLEARN) (3200,TensorFlow) (446,Theano) (609,Torch)};

\legend{Stackoverflow questions, Github  subscribers}

\end{axis}
\end{tikzpicture}

\end{figure}
\end{frame}

\begin{frame}{Conclusion and future work}
Conclusions:
\begin{itemize}
\item Reasonable performance
\item CNN is an excellent research tool
\end{itemize}
\vskip 0.2in
Future work:
\begin{itemize}
\item Fully convolutional networks with input of variable size
\item Alternative ways to expand the training set
\item Late fusing
\end{itemize}
\end{frame}


\end{document}