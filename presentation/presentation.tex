\documentclass{beamer}
\usetheme{Boadilla}
\usecolortheme{whale}

\usepackage{tikz}
\tikzset{
  font={\fontsize{8pt}{10}\selectfont}}
\usetikzlibrary{calc}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{matrix}

\definecolor{clstackoverflow}{HTML}{FE7A15}  
\definecolor{clgithub}{HTML}{4078C0} 

\graphicspath{ {../img/} }

\title{Convolutional neural networks
for classification of transmission
electron microscopy imagery}

\subtitle{}

\author{Sergii Gryshkevych}
\institute{
Uppsala University \\
Supervisor: Max Pihlstr{\"o}m \\
Reviewer: Ida-Maria Sintorn \\
Examinator: Justin Pearson \\
Opponent: Christopher Lagerhult
}
\date{December 14, 2016}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction}


\begin{frame}
\frametitle{Introduction}
This MSc project is done in cooperation with Vironova AB.
\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{vironova_logo.jpg} 
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Objective}
\begin{itemize}
\item<1-> Discuss the suitability of applying CNN method for classification of electron microscopy images
\item<2-> Benchmark CNN models against SVM classifier for the selected problems 
\item<3-> Discuss Deep Learning software:
\begin{itemize}
\item OS availability
\item Licenses
\item Performance
\item Community support
\end{itemize} 
\end{itemize}
\end{frame}


%
%	Problem description
%

\begin{frame}
\frametitle{Problem description: Lamellarity}

\begin{columns}
\column{0.35\textwidth}
Determine structure of a liposome according to the number of lamellae.
\vskip 0.2in
There are 14169 EM images and three classes:
\begin{itemize}
\item Unilamellar \\ 12368, 87.29\%
\item Multilamellar \\ 1717, 12.12\%
\item Uncertain \\ 84, 0.5\%
\end{itemize}
\column{0.65\textwidth}
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain2} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uni3} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/multi3} & \includegraphics[height=2cm, keepaspectratio]{problem_description/lamellarity/uncertain3} \\
Unilamellar & Multilamellar & Uncertain \\[6pt]
\end{tabular}
\end{figure}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Problem description: Encapsulation}

\begin{columns}
\column{0.35\textwidth}
Determine presence of a liposomal encapsulation.
\vskip 0.2in
There are $24918$ EM images and three classes:
\begin{itemize}
\item Full \\ 24255, 97.34\%
\item Empty \\ 161, 2.01\%
\item Uncertain \\ 502, 0.65\%
\end{itemize}
\column{0.65\textwidth}
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	\includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/full} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/empty} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/uncertain} \\
	\includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/full2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/empty2} & \includegraphics[height=2cm, keepaspectratio]{problem_description/packiging/uncertain2} \\
Full & Empty & Uncertain \\[6pt]
\end{tabular}
\end{figure}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{The data set of image features}

\begin{definition}
In computer vision and image processing, a feature is a piece of information which is relevant for solving the computational task related to a certain application. Features may be specific structures in the image such as points, edges or objects. Features may also be the result of a general neighborhood operation or feature detection applied to the image.
\end{definition}

\begin{itemize}
\item Maximum width
\item Diameter
\item Length
\item Histogram
\item Image Moments 
\item Radial Density Profile
\item Edge Density Profile
\item Signal to noise
\end{itemize}

\end{frame}

%
%	Convolutional neural networks
%

\begin{frame}
\frametitle{Convolutional neural networks}
\begin{definition}

\end{definition}
\end{frame}

%
%	The Class Imbalance Problem
%

\begin{frame}
\frametitle{The Class Imbalance Problem}

Lamellarity and Encapsulation data sets are imbalanced and it is a problem!
\vskip 0.2in
How to mitigate the class imbalance problem?
\begin{itemize}
\item Oversampling
\item Undersampling
\item SMOTE (Synthetic Minority Oversampling Technique)
\item Artificial data
\item Higher penalties for misclassification of minority classes
\end{itemize}


\end{frame}

%
%	Regularization
%

\begin{frame}
\frametitle{Regularization}

\begin{definition}
\textit{Regularization} is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error
\end{definition}

\begin{itemize}
\item Weight decay
\item Noise injection: label smoothing
\item Dropout
\item Early stopping
\item Data augmentation
\end{itemize}

\end{frame}

%
%	Data augmentation
%

\section{Data augmentation}
\begin{frame}
\frametitle{Data augmentation techniques}

Data augmentation introduces additional diversity in the training set by applying different transformations to the existing examples.
\vskip 0.2in
\begin{itemize}
\item Rotation in the range $[-180, 180]$ degrees with spline interpolation
\item Shear transformation in the range $[0, 0.2]$
\item Vertical shift in the range $[-10, 10]$ percent of total height
\item Horizontal shift in the range $[-10, 10]$ percent of total width
\item Zoom in the range $[0.8, 1.0]$ which means zoom by a maximum $20\%$
\item Horizontal flip
\item Vertical flip
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Data augmentation example}
\begin{figure}
\centering
\begin{tabular}{ccccc}
\includegraphics[scale=0.5]{augmented/541253.jpg} & \includegraphics[scale=0.5]{augmented/_0_645.jpeg} & \includegraphics[scale=0.5]{augmented/_0_1385.jpeg} & \includegraphics[scale=0.5]{augmented/_0_1749.jpeg} & \includegraphics[scale=0.5]{augmented/_0_2343.jpeg} \\

\includegraphics[scale=0.5]{augmented/_0_4050.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3413.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3414.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3687.jpeg} & \includegraphics[scale=0.5]{augmented/_0_3916.jpeg} \\
	
\includegraphics[scale=0.5]{augmented/_0_4165.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5014.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5165.jpeg} & \includegraphics[scale=0.5]{augmented/_0_5567.jpeg} & \includegraphics[scale=0.5]{augmented/_0_6089.jpeg} \\

\includegraphics[scale=0.5]{augmented/_0_7140.jpeg} & \includegraphics[scale=0.5]{augmented/_0_7746.jpeg} & \includegraphics[scale=0.5]{augmented/_0_8553.jpeg} & \includegraphics[scale=0.5]{augmented/_0_8763.jpeg} & \includegraphics[scale=0.5]{augmented/_0_9361.jpeg} 
	
\end{tabular}
\end{figure}
\end{frame}

%
%	Performance measures
%

\begin{frame}
\frametitle{Performance measures}

\begin{block}{The Accuracy Paradox}
Models with a given accuracy may have greater predictive power than models with higher accuracy.
\end{block}

\begin{block}{Confusion matrix}
\begin{table}
\begin{tabular}{|c|c|c|}
\hline 
 & Predicted True & Predicted False \\ 
\hline 
Actual True & True Positive & False Negative \\ 
\hline 
Actual False & False Positive & True Negative \\ 
\hline 
\end{tabular} 
\end{table}
\end{block}


\end{frame}

\begin{frame}
\frametitle{Performance measures}

\begin{block}<1->{True positive rate (TPR)}
AKA sensitivity or recall: $TPR = \frac{TP}{TP + FN}$
\end{block}

\begin{block}<2->{True negative rate (TNR)}
AKA specificity: $TNR = \frac{TN}{TN + FP}$
\end{block}

\begin{block}<3->{Positive predicted value (PPV)}
AKA precision: $PPV = \frac{TP}{TP + FP}$
\end{block}

\begin{block}<4->{Negative predicted value (NPV)}
$NPV = \frac{TN}{TN + FN}$
\end{block}

\begin{block}<5->{$F_1$ score}
It is a harmonic mean of TPR and TNR
\end{block}

\end{frame}

%
%	LipNet networks
%

\begin{frame}
\frametitle{Network architectures}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{lipnet_architecture.png} 
\end{figure}

\end{frame}

\section{Results}
\begin{frame}
\frametitle{Which LipNet model is the best?}
Five LipNet models are evaluated by recording their 5-fold cross validated $F_1$ scores. 
\vskip 0.2in
\begin{itemize}
\item<2-> The Lamellarity Problem: \textbf{LipNet-4} is the best
\item<3-> The Encapsulation Problem: There is no clear leader. \textbf{LipNet-2} is selected for the final experiment. LipNet-6 performed worst. 
\end{itemize}
\end{frame}

%
%	Effect of data augmentation
%

\begin{frame}
\frametitle{Effect of the data augmentation}

\begin{figure}
\centering

\begin{tikzpicture}
\begin{groupplot}[
	group style = {group size = 1 by 2, vertical sep=1cm,},
	width = 12cm,
	height = 4.5cm,
	grid=major,
    ymin=0.0,
    ymax=1.0,
    %ylabel=Average normalized true positive rate, 
    legend pos=south east,
    symbolic x coords={None, Rotation, Shift, Flip, Shear, Zoom},
    xtick=data,
    ]

% lamellarity
\nextgroupplot
\addplot+ [mark=*] coordinates{(None, 0.992) (Rotation, 0.966) (Shift, 0.906) (Flip, 0.888) (Shear, 0.896) (Zoom, 0.84)};
\addplot+ [mark=square*] coordinates{(None, 0.958) (Rotation, 0.99) (Shift, 0.986) (Flip, 0.988) (Shear, 0.988) (Zoom, 0.988)};
\addplot+ [mark=triangle*] coordinates{(None, 0.0) (Rotation, 0.346) (Shift, 0.618) (Flip, 0.632) (Shear, 0.606) (Zoom, 0.596)};

\legend{Unilamellar, Multilamellar, Uncertain}

% encapsulation
\nextgroupplot
\addplot+ [mark=*] coordinates{(None, 0.18) (Rotation, 0.41) (Shift, 0.53) (Flip, 0.57) (Shear, 0.59) (Zoom, 0.54)};
\addplot+ [mark=square*] coordinates{(None, 0.99) (Rotation, 0.95) (Shift, 0.89) (Flip, 0.89) (Shear, 0.89) (Zoom, 0.73)};
\addplot+ [mark=triangle*] coordinates{(None, 0.25) (Rotation, 0.55) (Shift, 0.64) (Flip, 0.62) (Shear, 0.6) (Zoom, 0.74)};
\legend{Empty, Full, Uncertain}

\end{groupplot}
\end{tikzpicture}
\end{figure}

\end{frame}

%
%	Input images modes
%

\begin{frame}
\frametitle{Input images: surrounding and masking}
Each image contains a liposome object and its surrounding which goes 50 pixels in each direction. Corresponding particle masks are also available. 
\vskip 0.2in
Three choices:
\begin{itemize}
\item Images with surrounding
\item Cropped images
\item \alert<2>{Cropped and masked} 
\end{itemize}
\end{frame}

%
%	CNN vs SVM
%

\section{Benchmarking}

\begin{frame}
\frametitle{CNN vs SVM: Lamellarity}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{cnn_vs_svm_lamellarity.png} 
\end{figure}

\begin{itemize}
\item CNN is better in predicting \textit{unilamellar} and \textit{multilamellar}.
\item More false negative \textit{unilamellar} by SVM than CNN.
\item More false positive \textit{multilamellar} by SVM than by CNN.
\item SVM is better in predicting \textit{uncertain}.
\item Many false positive predictions of \textit{uncertain}, mainly \textit{unilamellar} is confused with \textit{uncertain}.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{CNN vs SVM: Encapsulation}

\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{cnn_vs_svm_encapsulation.png} 
\end{figure}

\begin{itemize}
\item SVM is slightly better than CNN.
\item Poor precision for \textit{empty} and \textit{uncertain} because some \textit{full} are falsely classified.
\item Low NPV for \textit{full}, it is a direct consequence of the previous item.
\item Precision of \textit{full} and NPV of \textit{empty} and \textit{uncertain} are almost 1, so hardly any false positive of \textit{empty}.
\item Both classifiers underestimate the number of \textit{full}.
\end{itemize}

\end{frame}

%
%	Popularity of DL software
%

\begin{frame}
\frametitle{Popularity of deep learning software as of October 2016}
\begin{figure}[H]
\centering

\begin{tikzpicture}
\begin{axis}[
	xbar,
	y axis line style = { opacity = 0 },
	axis x line = none,
	tickwidth = 0pt,
	enlarge y limits = 0.2,
	enlarge x limits = 0.02,
	bar width = 0.2cm,
	symbolic y coords = {Caffe, CNTK, Keras, Lasagne, MXNET, NOLEARN, TensorFlow, Theano, Torch},
	ytick = data,
	nodes near coords,
	height = 9cm,
	legend style={at={(1.1, 0.5)},anchor=north east}
    ]

% stackoverflow
\addplot[color=clstackoverflow, fill=clstackoverflow] coordinates{(1161,Caffe) (15,CNTK) (571,Keras) (168,Lasagne) (29,MXNET) (45,NOLEARN) (4000,TensorFlow) (1555,Theano) (533,Torch)};

% github
\addplot[color=clgithub, fill=clgithub] coordinates{(1605,Caffe) (746,CNTK) (667,Keras) (208,Lasagne) (615,MXNET) (54,NOLEARN) (3200,TensorFlow) (446,Theano) (609,Torch)};

\legend{Stackoverflow questions, Github  subscribers}

\end{axis}
\end{tikzpicture}

\end{figure}
\end{frame}

\end{document}